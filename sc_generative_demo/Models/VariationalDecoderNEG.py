# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/12_Model_Variational_Decoder_NEG.ipynb.

# %% auto 0
__all__ = ['VariationalDecoderBI']

# %% ../../nbs/12_Model_Variational_Decoder_NEG.ipynb 3
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

class VariationalDecoderBI(nn.Module):
    """Variational Decoder Negative binomial""" 
    def __init__(self, input_size, hidden_sizes, output_size, dropout, use_norm):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_sizes = hidden_sizes
        self.dropout = dropout
        self.use_batch_norm = use_norm
        
        
        # create a list of layers
        layers = []

        # input layer
        layers.append(nn.Linear(self.input_size, self.hidden_sizes[0]))
        layers.append(nn.LeakyReLU(0.2))
        if self.dropout > 0:
            layers.append(nn.Dropout(p=self.dropout))

        # hidden layers
        for i in range(1, len(self.hidden_sizes)):
            layers.append(nn.Linear(self.hidden_sizes[i-1], self.hidden_sizes[i]))
            if self.use_batch_norm:
                layers.append(nn.InstanceNorm1d(self.hidden_sizes[i]))
            layers.append(nn.LeakyReLU(0.2))
            if self.dropout > 0:
                layers.append(nn.Dropout(p=self.dropout))
        
        # output layer
    

        # create the model using Sequential
        self.model = nn.Sequential(*layers)
        self.mu_layer = nn.Linear(self.hidden_sizes[-1], self.output_size)
        self.disp_layer = nn.Linear(self.hidden_sizes[-1], self.output_size)
       

    def forward(self, x):
        x = self.model(x)
        mu = self.mu_layer(x)
        disp = self.disp_layer(x)
        mu = F.softplus(mu)
        disp = F.softplus(disp)
        return mu, disp
      


