{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eamonmca/sc_generative_demo/blob/master/Wasserstein_GAN_GP_Full_Evaluation_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yKRnz3qrnNXo",
        "outputId": "f2589069-6e11-4b9f-9986-fc0a07a6a2ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  /usr/bin/python3 -m pip uninstall [options] <package> ...\n",
            "  /usr/bin/python3 -m pip uninstall [options] -r <requirements file> ...\n",
            "\n",
            "no such option: -Y\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==3.1.3\n",
            "  Using cached matplotlib-3.1.3-cp39-cp39-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.1.3) (1.22.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.1.3) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.16.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scanpy 1.9.3 requires matplotlib>=3.4, but you have matplotlib 3.1.3 which is incompatible.\n",
            "plotnine 0.10.1 requires matplotlib>=3.5.0, but you have matplotlib 3.1.3 which is incompatible.\n",
            "mizani 0.8.1 requires matplotlib>=3.5.0, but you have matplotlib 3.1.3 which is incompatible.\n",
            "arviz 0.15.1 requires matplotlib>=3.2, but you have matplotlib 3.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.14.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (67.6.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.19.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scanpy in /usr/local/lib/python3.9/dist-packages (1.9.3)\n",
            "Requirement already satisfied: h5py>=3 in /usr/local/lib/python3.9/dist-packages (from scanpy) (3.8.0)\n",
            "Requirement already satisfied: session-info in /usr/local/lib/python3.9/dist-packages (from scanpy) (1.0.0)\n",
            "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.9/dist-packages (from scanpy) (1.10.1)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.9/dist-packages (from scanpy) (0.5.3)\n",
            "Requirement already satisfied: numba>=0.41.0 in /usr/local/lib/python3.9/dist-packages (from scanpy) (0.56.4)\n",
            "Collecting matplotlib>=3.4\n",
            "  Using cached matplotlib-3.7.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.9/dist-packages (from scanpy) (1.2.2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.9/dist-packages (from scanpy) (1.5.3)\n",
            "Requirement already satisfied: statsmodels>=0.10.0rc2 in /usr/local/lib/python3.9/dist-packages (from scanpy) (0.13.5)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.9/dist-packages (from scanpy) (8.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from scanpy) (23.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.9/dist-packages (from scanpy) (3.1)\n",
            "Requirement already satisfied: anndata>=0.7.4 in /usr/local/lib/python3.9/dist-packages (from scanpy) (0.9.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from scanpy) (1.2.0)\n",
            "Requirement already satisfied: umap-learn>=0.3.10 in /usr/local/lib/python3.9/dist-packages (from scanpy) (0.5.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.9/dist-packages (from scanpy) (0.12.2)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from scanpy) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from scanpy) (4.65.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.4->scanpy) (1.4.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.41.0->scanpy) (67.6.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.41.0->scanpy) (0.39.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0->scanpy) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22->scanpy) (3.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy->scanpy) (1.16.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.9/dist-packages (from umap-learn>=0.3.10->scanpy) (0.5.8)\n",
            "Requirement already satisfied: stdlib-list in /usr/local/lib/python3.9/dist-packages (from session-info->scanpy) (0.8.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.4->scanpy) (3.15.0)\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.1.3\n",
            "    Uninstalling matplotlib-3.1.3:\n",
            "      Successfully uninstalled matplotlib-3.1.3\n",
            "Successfully installed matplotlib-3.7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        " %matplotlib inline\n",
        "!python -m pip uninstall matplotlib -Y\n",
        "!pip install matplotlib==3.1.3 \n",
        "!pip install wandb\n",
        "!pip install scanpy\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "andwqcpJmgZO",
        "outputId": "0d455ba8-c6e9-4594-c7ed-30d82cb50b68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f1304470c10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np  # Thinly-wrapped numpy\n",
        "import torch.autograd as autograd\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import scanpy as sc\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "import sklearn.preprocessing\n",
        "import sklearn.model_selection\n",
        "from tqdm import tqdm\n",
        "from scipy import sparse\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fyeiaHCwmgZQ"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "def get_device_and_gmount():\n",
        "    # Get the operating system and version\n",
        "    os = platform.system()\n",
        "    version = platform.release()\n",
        "\n",
        "    # Get the machine's architecture\n",
        "    arch = platform.machine()\n",
        "\n",
        "    # Set the default renderer based on the operating system\n",
        "    if os == 'Darwin':\n",
        "        pio.renderers.default = 'notebook'\n",
        "        print(\"Using Apple MPS on Macbook Pro\")\n",
        "    \n",
        "    elif os == 'Linux':\n",
        "        pio.renderers.default = 'colab'\n",
        "        print(\"Using Colab on Linux\")\n",
        "\n",
        "    # Set the device based on the machine's architecture\n",
        "    if arch == 'x86_64':\n",
        "        device = torch.device('mps') if os == 'Darwin' else torch.device('cuda')\n",
        "        gmount = True if os == 'Linux' else False\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        gmount = False\n",
        "\n",
        "    print(\"Using device:\", device)\n",
        "    \n",
        "    return device, gmount\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jn5HT1RmgZR",
        "outputId": "64c70bb9-d8cd-43c1-b99f-5eec5a43fdc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab on Linux\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device, gmount = get_device_and_gmount()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFszDCKQmgZR",
        "outputId": "19c19d89-9d0d-4a44-c8e3-eeda77dc9c76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if gmount:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    scdata = sc.read_h5ad(\"/content/drive/MyDrive/scintegration/GEX.h5ad\")\n",
        "else:\n",
        "    scdata = sc.read_h5ad(\"/Users/eamonmcandrew/Desktop/Single_cell_integration/Data/Multi-ome/GEX.h5ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jhq8CedvwcnO",
        "outputId": "716f43e1-7165-4334-fac9-b5a44396b067"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AnnData object with n_obs × n_vars = 69249 × 13431\n",
              "    obs: 'GEX_pct_counts_mt', 'GEX_n_counts', 'GEX_n_genes', 'GEX_size_factors', 'GEX_phase', 'ATAC_nCount_peaks', 'ATAC_atac_fragments', 'ATAC_reads_in_peaks_frac', 'ATAC_blacklist_fraction', 'ATAC_nucleosome_signal', 'cell_type', 'batch', 'ATAC_pseudotime_order', 'GEX_pseudotime_order', 'Samplename', 'Site', 'DonorNumber', 'Modality', 'VendorLot', 'DonorID', 'DonorAge', 'DonorBMI', 'DonorBloodType', 'DonorRace', 'Ethnicity', 'DonorGender', 'QCMeds', 'DonorSmoker'\n",
              "    var: 'feature_types', 'gene_id'\n",
              "    uns: 'ATAC_gene_activity_var_names', 'dataset_id', 'genome', 'organism'\n",
              "    obsm: 'ATAC_gene_activity', 'ATAC_lsi_full', 'ATAC_lsi_red', 'ATAC_umap', 'GEX_X_pca', 'GEX_X_umap'\n",
              "    layers: 'counts'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "scdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z9xdgMeaweis"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l8nIHsqGmgZR"
      },
      "outputs": [],
      "source": [
        "def stratified_split(data, test_size, random_state, split_criteria):\n",
        "    \"\"\"\n",
        "    Splits the data into train and test sets stratified by the batch column\n",
        "    \"\"\"\n",
        "    train = []\n",
        "    test = []\n",
        "    for batch in data.obs[split_criteria].unique():\n",
        "        batch_data = data[data.obs[split_criteria] == batch]\n",
        "        batch_train, batch_test = sklearn.model_selection.train_test_split(batch_data, test_size=test_size, random_state=random_state)\n",
        "        batch_train, batch_test = list(batch_train.obs.index), list(batch_test.obs.index)\n",
        "        train.extend(batch_train)\n",
        "        test.extend(batch_test)\n",
        "        \n",
        "    return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re9X5gmLmgZS",
        "outputId": "104bc556-18f0-4179-e48a-6b80176a2c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if gmount == True:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    scdata = sc.read_h5ad(\"/content/drive/MyDrive/scintegration/GEX.h5ad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "cT6PVLo8mgZS",
        "outputId": "75d31354-8fd3-47d0-d57c-532320607f5b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gcP6QkdbmgZS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import sklearn.preprocessing\n",
        "\n",
        "class GEX_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, scaler=None, cat_var=None, label_encoder=None):\n",
        "        self.data = data\n",
        "        self.values = np.asarray(data.X.todense())\n",
        "        self.cat_var = cat_var\n",
        "\n",
        "        label_encoder_functions = {\n",
        "            \"numeric\": lambda: torch.tensor(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]), dtype=torch.long),\n",
        "            \"range_map\": lambda: sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]).reshape(-1, 1),\n",
        "            \"one_hot\": lambda: sklearn.preprocessing.OneHotEncoder().fit_transform(sklearn.preprocessing.LabelEncoder().fit_transform(self.data.obs[self.cat_var]).reshape(-1, 1)).toarray()\n",
        "        }\n",
        "\n",
        "        if label_encoder in label_encoder_functions:\n",
        "            cat_var_data = label_encoder_functions[label_encoder]()\n",
        "            if label_encoder == \"range_map\":\n",
        "                cat_var_data = torch.tensor(sklearn.preprocessing.MinMaxScaler().fit_transform(cat_var_data), dtype=torch.float32)\n",
        "            elif label_encoder == \"one_hot\":\n",
        "                cat_var_data = torch.tensor(cat_var_data, dtype=torch.float32)\n",
        "            self.cat_var_data = cat_var_data\n",
        "        else:\n",
        "            self.cat_var_data = None\n",
        "\n",
        "        scaler_functions = {\n",
        "            \"Standard\": lambda: (sklearn.preprocessing.StandardScaler().fit(self.values), sklearn.preprocessing.StandardScaler().fit_transform(self.values)),\n",
        "            \"MinMax\": lambda: (sklearn.preprocessing.MinMaxScaler().fit(self.values), sklearn.preprocessing.MinMaxScaler().fit_transform(self.values))\n",
        "        }\n",
        "\n",
        "        if scaler in scaler_functions:\n",
        "            self.scaler_obj, scaled_values = scaler_functions[scaler]()\n",
        "            self.scaled_values = torch.tensor(scaled_values, dtype=torch.float32)\n",
        "        else:\n",
        "            self.scaled_values = torch.tensor(self.values, dtype=torch.float32)\n",
        "            self.scaler_obj = None\n",
        "\n",
        "    @property\n",
        "    def n_features(self):\n",
        "        return self.values.shape[1]\n",
        "\n",
        "    @property\n",
        "    def n_catagories(self):\n",
        "        return self.cat_var_data.shape[1] if self.cat_var_data is not None else 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.scaled_values[idx], self.cat_var_data[idx]\n",
        "\n",
        "    def inverse_transform(self, scaled_data):\n",
        "        if self.scaler_obj is not None:\n",
        "            data = self.scaler_obj.inverse_transform(scaled_data)\n",
        "            data = np.asarray(data)\n",
        "        else:\n",
        "            data = np.asarray(scaled_data)\n",
        "        return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cANaD_q4-XD8"
      },
      "outputs": [],
      "source": [
        "def stratified_split(data, test_size, random_state, split_criteria):\n",
        "    \"\"\"\n",
        "    Splits the data into train and test sets stratified by the batch column\n",
        "    \"\"\"\n",
        "    train = []\n",
        "    test = []\n",
        "    for batch in data.obs[split_criteria].unique():\n",
        "        batch_data = data[data.obs[split_criteria] == batch]\n",
        "        batch_train, batch_test = sklearn.model_selection.train_test_split(batch_data, test_size=test_size, random_state=random_state)\n",
        "        batch_train, batch_test = list(batch_train.obs.index), list(batch_test.obs.index)\n",
        "        train.extend(batch_train)\n",
        "        test.extend(batch_test)\n",
        "        \n",
        "    return train, test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fOkl1P4wmgZT"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, dropout, hidden_sizes, output_size, use_batch_norm):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.dropout = dropout\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        # create a list of layers\n",
        "        layers = []\n",
        "\n",
        "        # input layer\n",
        "        layers.append(nn.Linear(self.input_size, self.hidden_sizes[0]))\n",
        "        if self.use_batch_norm:\n",
        "            layers.append(nn.BatchNorm1d(self.hidden_sizes[0]))\n",
        "        layers.append(nn.ReLU())\n",
        "        if self.dropout > 0:\n",
        "            layers.append(nn.Dropout(p=self.dropout))\n",
        "\n",
        "        # hidden layers\n",
        "        for i in range(1, len(self.hidden_sizes)):\n",
        "            layers.append(nn.Linear(self.hidden_sizes[i-1], self.hidden_sizes[i]))\n",
        "            if self.use_batch_norm:\n",
        "                layers.append(nn.BatchNorm1d(self.hidden_sizes[i]))\n",
        "            layers.append(nn.ReLU())\n",
        "            if self.dropout > 0:\n",
        "                layers.append(nn.Dropout(p=self.dropout))\n",
        "\n",
        "        # output layer\n",
        "        layers.append(nn.Linear(self.hidden_sizes[-1], self.output_size))\n",
        "        \n",
        "\n",
        "        # create the model using Sequential\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yD-8i06-mgZT"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, input_size, dropout, hidden_sizes, output_size, use_batch_norm):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.dropout = dropout\n",
        "    self.hidden_sizes = hidden_sizes\n",
        "    self.output_size = output_size\n",
        "    self.use_batch_norm = use_batch_norm\n",
        "\n",
        "    # create a list of layers\n",
        "    layers = []\n",
        "\n",
        "    # input layer\n",
        "    layers.append(nn.Linear(self.input_size, self.hidden_sizes[0]))\n",
        "    layers.append(nn.LeakyReLU(0.2))\n",
        "    if self.dropout > 0:\n",
        "      layers.append(nn.Dropout(p=self.dropout))\n",
        "\n",
        "    # hidden layers\n",
        "    for i in range(1, len(self.hidden_sizes)):\n",
        "      layers.append(nn.Linear(self.hidden_sizes[i-1], self.hidden_sizes[i]))\n",
        "      if self.use_batch_norm:\n",
        "        layers.append(nn.InstanceNorm1d(self.hidden_sizes[i]))\n",
        "      layers.append(nn.LeakyReLU(0.2))\n",
        "      if self.dropout > 0:\n",
        "        layers.append(nn.Dropout(p=self.dropout))\n",
        "\n",
        "    # output layer\n",
        "    layers.append(nn.Linear(self.hidden_sizes[-1], self.output_size))\n",
        "\n",
        "    # create the model using Sequential\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "k2PgGPi1Y66a"
      },
      "outputs": [],
      "source": [
        "class critic(nn.Module):\n",
        "  def __init__(self, input_size, dropout, hidden_sizes, output_size, use_batch_norm):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.dropout = dropout\n",
        "    self.hidden_sizes = hidden_sizes\n",
        "    self.output_size = output_size\n",
        "    self.use_batch_norm = use_batch_norm\n",
        "\n",
        "    # create a list of layers\n",
        "    layers = []\n",
        "\n",
        "    # input layer\n",
        "    layers.append(nn.Linear(self.input_size, self.hidden_sizes[0]))\n",
        "    layers.append(nn.LeakyReLU(0.2))\n",
        "    if self.dropout > 0:\n",
        "      layers.append(nn.Dropout(p=self.dropout))\n",
        "\n",
        "    # hidden layers\n",
        "    for i in range(1, len(self.hidden_sizes)):\n",
        "      layers.append(nn.Linear(self.hidden_sizes[i-1], self.hidden_sizes[i]))\n",
        "      if self.use_batch_norm:\n",
        "        layers.append(nn.nn.LayerNorm(self.hidden_sizes[i]))\n",
        "      layers.append(nn.LeakyReLU(0.2))\n",
        "      if self.dropout > 0:\n",
        "        layers.append(nn.Dropout(p=self.dropout))\n",
        "\n",
        "    # output layer\n",
        "    layers.append(nn.Linear(self.hidden_sizes[-1], self.output_size))\n",
        "\n",
        "    # create the model using Sequential\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DDKHovIomgZT"
      },
      "outputs": [],
      "source": [
        "# Define the GAN model\n",
        "class GAN(nn.Module):\n",
        "    def __init__(self, generator, discriminator):\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.discriminator(self.generator(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XSIDCxDX0res"
      },
      "outputs": [],
      "source": [
        "def get_noise(n_samples, z_dim, device=device):\n",
        "    '''\n",
        "    Function for creating noise vectors: Given the dimensions (n_samples, z_dim),\n",
        "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
        "    Parameters:\n",
        "        n_samples: the number of samples to generate, a scalar\n",
        "        z_dim: the dimension of the noise vector, a scalar\n",
        "        device: the device type\n",
        "    '''\n",
        "    return torch.randn(n_samples,z_dim,device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HoK2bQef5Tkg"
      },
      "outputs": [],
      "source": [
        "def generate_fake_data_and_combine(original_data, generator, z_dim, batch_size):\n",
        "    # Store original batch and cell_type information\n",
        "    original_batch_info = original_data.obs['batch'].copy()\n",
        "    original_cell_type_info = original_data.obs['cell_type'].copy()\n",
        "    \n",
        "    generator.eval()\n",
        "    noise = get_noise(batch_size, z_dim, device=device)\n",
        "    fake_data = generator(noise)\n",
        "    fake_adata = sc.AnnData(fake_data.detach().cpu().numpy())\n",
        "\n",
        "    # add an index column to the fake data, starting at the number of real cells\n",
        "    fake_adata.var.index = original_data.var.index\n",
        "    adata = sc.AnnData.concatenate(original_data, fake_adata, batch_key='group', batch_categories=['real', 'fake'])\n",
        "\n",
        "    # Reassign original batch and cell_type information\n",
        "    adata.obs.loc[adata.obs['group'] == 'real', 'batch'] = original_batch_info.values\n",
        "    adata.obs.loc[adata.obs['group'] == 'real', 'cell_type'] = original_cell_type_info.values\n",
        "\n",
        "    # Convert 'batch' and 'cell_type' columns to categorical dtype if not already\n",
        "    if not pd.api.types.is_categorical_dtype(adata.obs['batch']):\n",
        "        adata.obs['batch'] = adata.obs['batch'].astype('category')\n",
        "    if not pd.api.types.is_categorical_dtype(adata.obs['cell_type']):\n",
        "        adata.obs['cell_type'] = adata.obs['cell_type'].astype('category')\n",
        "\n",
        "    # Add \"fake\" as a category for 'cell_type' column\n",
        "    adata.obs['cell_type'] = adata.obs['cell_type'].cat.add_categories('fake')\n",
        "\n",
        "    # set the \"cell_type\" columns for the fake cells to \"fake\"\n",
        "    adata.obs.loc[adata.obs['group'] == 'fake', 'cell_type'] = 'fake'\n",
        "\n",
        "       # Add \"fake\" as a category for 'cell_type' column\n",
        "    adata.obs['batch'] = adata.obs['batch'].cat.add_categories('fake')\n",
        "\n",
        "    # set the \"cell_type\" columns for the fake cells to \"fake\"\n",
        "    adata.obs.loc[adata.obs['group'] == 'fake', 'batch'] = 'fake'\n",
        "\n",
        "    # compute the UMAP of the combined data\n",
        "    sc.pp.neighbors(adata, n_neighbors=10)\n",
        "    sc.tl.umap(adata)\n",
        "\n",
        "    return adata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HK5Dw5X4uk95"
      },
      "outputs": [],
      "source": [
        "import scanpy as sc\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def umap_with_highlighted_category(adata, category_column, highlight_category, highlight_color=(0, 0, 1, 1)):\n",
        "    # Get unique categories from the specified column\n",
        "    unique_categories = adata.obs[category_column].cat.categories.tolist()\n",
        "\n",
        "    # Add the 'fake' category to the unique_categories list\n",
        "    unique_categories.append(highlight_category)\n",
        "\n",
        "    # Create a custom color map for the unique categories, using the 'viridis' colormap\n",
        "    cmap = plt.cm.get_cmap('viridis', len(unique_categories))\n",
        "\n",
        "    # Prepare color map dictionary, where key is category, and value is color\n",
        "    cmap_dict = {cat: cmap(i) for i, cat in enumerate(unique_categories)}\n",
        "\n",
        "    # Change the color of the highlight category to the specified highlight color\n",
        "    cmap_dict[highlight_category] = highlight_color\n",
        "\n",
        "    return cmap_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EuXO3mOtBQmG"
      },
      "outputs": [],
      "source": [
        "def plot_umap_by_group_labels_on(adata, color, title=None):\n",
        "    plot = sc.pl.umap(adata, color=color, legend_loc='on data', title=title, show=False)\n",
        "    return plot\n",
        "\n",
        "def plot_umap_by_group_labels(adata, color, cmap_dict, title=None):\n",
        "       # Plot the UMAP with the custom color map\n",
        "    plot = sc.pl.umap(adata, color=color, groups=adata.obs.batch.unique(), palette=cmap_dict, legend_fontsize=12, show = False)\n",
        "    return plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MlNwUdvsWmZE"
      },
      "outputs": [],
      "source": [
        "def jaccard_index(set_a, set_b):\n",
        "    \"\"\"\n",
        "    Calculate the Jaccard index between two sets.\n",
        "\n",
        "    Args:\n",
        "        set_a: A set of highly variable genes in dataset A.\n",
        "        set_b: A set of highly variable genes in dataset B.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the raw Jaccard index and the scaled Jaccard index (0 to 1).\n",
        "    \"\"\"\n",
        "    intersection = set_a.intersection(set_b)\n",
        "    union = set_a.union(set_b)\n",
        "\n",
        "    # Calculate the Jaccard index\n",
        "    jaccard = len(intersection) / len(union)\n",
        "\n",
        "    return jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "alcHYZI2O1p0"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import hypergeom\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def get_hvg_set(adata, n_top_genes=2000):\n",
        "    adata_copy = adata.copy()\n",
        "    if isinstance(adata_copy.X, csr_matrix):\n",
        "        adata_copy.X = csr_matrix(adata_copy.X.maximum(0).toarray() + 1e-10)\n",
        "    else:\n",
        "        adata_copy.X = np.clip(adata_copy.X, 0, None)\n",
        "\n",
        "    sc.pp.log1p(adata_copy)\n",
        "    sc.pp.highly_variable_genes(adata_copy, n_top_genes=n_top_genes)\n",
        "    hvg_set = set(adata_copy.var_names[adata_copy.var['highly_variable']])\n",
        "    return hvg_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UWOCsgNYgfod"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scanpy as sc\n",
        "from scipy.stats import hypergeom\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def jaccard_index(set1, set2):\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union\n",
        "\n",
        "def hvg_overlap_analysis(hvg_set_1, hvg_set_2, adata, dataset, n_top_genes):\n",
        "    adata_copy = adata.copy()\n",
        "\n",
        "    if isinstance(adata_copy.X, csr_matrix):\n",
        "        adata_copy.X = dataset.inverse_transform(adata_copy.X.toarray())\n",
        "    else:\n",
        "        adata_copy.X = dataset.inverse_transform(adata_copy.X)\n",
        "\n",
        "    if isinstance(adata_copy.X, csr_matrix):\n",
        "        adata_copy.X = csr_matrix(adata_copy.X.maximum(0).toarray() + 1e-10)\n",
        "    else:\n",
        "        adata_copy.X = np.clip(adata_copy.X, 0, None)\n",
        "\n",
        "    sc.pp.log1p(adata_copy)\n",
        "\n",
        "    # Normalize and find HVGs for each dataset\n",
        "    sc.pp.highly_variable_genes(adata_copy, n_top_genes=n_top_genes)\n",
        "\n",
        "    # Calculate the overlap between the HVG sets\n",
        "    hvg2 = set(adata_copy.var_names[adata_copy.var['highly_variable']])\n",
        "\n",
        "    jaccard_train = jaccard_index(hvg_set_1, hvg2)\n",
        "    jaccard_test = jaccard_index(hvg_set_2, hvg2)\n",
        "\n",
        "    return jaccard_train, jaccard_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hfcdWwrz5hZo"
      },
      "outputs": [],
      "source": [
        "def train_gan_one_epoch(epoch, GEX_dataloader_train, gan, g_opt, d_opt, criterion, z_dim):\n",
        "    gan.generator.to(device)\n",
        "    gan.discriminator.to(device)\n",
        "    gan.train()\n",
        "    total_loss_discriminator = 0\n",
        "    total_loss_generator = 0\n",
        "    for iteration, (data, _) in enumerate(GEX_dataloader_train):\n",
        "        data = data.to(device)\n",
        "        # Generate fake data\n",
        "        noise = get_noise(data.shape[0], z_dim, device=device)\n",
        "        fake_data = gan.generator(noise)\n",
        "\n",
        "        fake_data = fake_data.to(device)\n",
        "\n",
        "        # Train the discriminator\n",
        "        d_opt.zero_grad()\n",
        "        pred_real = gan.discriminator(data)\n",
        "        pred_fake = gan.discriminator(fake_data.detach())\n",
        "        loss_real = criterion(pred_real, torch.ones_like(pred_real))\n",
        "        loss_fake = criterion(pred_fake, torch.zeros_like(pred_fake))\n",
        "        loss_discriminator = (loss_real + loss_fake) / 2\n",
        "        loss_discriminator.backward(retain_graph=True)\n",
        "        wandb.log({\"loss_discriminator\": loss_discriminator})\n",
        "        d_opt.step()\n",
        "\n",
        "        # Train the generator\n",
        "        g_opt.zero_grad()\n",
        "        noise = get_noise(data.shape[0], z_dim, device=device)\n",
        "        pred_fake = gan.discriminator(fake_data)\n",
        "        loss_generator = criterion(pred_fake, torch.ones_like(pred_fake))\n",
        "        loss_generator.backward()\n",
        "        wandb.log({\"loss_generator\": loss_generator})\n",
        "        g_opt.step()\n",
        "\n",
        "        total_loss_discriminator += loss_discriminator.item()\n",
        "        total_loss_generator += loss_generator.item()\n",
        "\n",
        "    num_iterations = len(GEX_dataloader_train)\n",
        "    mean_loss_discriminator = total_loss_discriminator / num_iterations\n",
        "    mean_loss_generator = total_loss_generator / num_iterations\n",
        "\n",
        "    return mean_loss_discriminator, mean_loss_generator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tfu6gm19LZ0R"
      },
      "outputs": [],
      "source": [
        "def train_gan_one_epoch_wasserstein(epoch, GEX_dataloader_train, gan, g_opt, d_opt, criterion, z_dim, clip_value, n_disc):\n",
        "    gan.generator.to(device)\n",
        "    gan.discriminator.to(device)\n",
        "    gan.train()\n",
        "    total_loss_discriminator = 0\n",
        "    total_loss_generator = 0\n",
        "    for iteration, (data, _) in enumerate(GEX_dataloader_train):    \n",
        "        data = data.to(device)\n",
        "        # Generate fake data\n",
        "        noise = get_noise(data.shape[0], z_dim, device=device)\n",
        "        fake_data = gan.generator(noise)\n",
        "\n",
        "        fake_data = fake_data.to(device)\n",
        "\n",
        "        # Train the discriminator\n",
        "        d_opt.zero_grad()\n",
        "        pred_real = gan.discriminator(data)\n",
        "        loss_discriminator = torch.mean(gan.discriminator(fake_data)) - torch.mean(gan.discriminator(data))\n",
        "        loss_discriminator.backward(retain_graph=True)\n",
        "        wandb.log({\"loss_discriminator\": loss_discriminator})\n",
        "        d_opt.step()\n",
        "        # clip weights of discriminator\n",
        "        for p in gan.discriminator.parameters():\n",
        "          p.data.clamp_(-clip_value, clip_value)\n",
        "\n",
        "        # Train the generator\n",
        "        if iteration % n_disc == 0:\n",
        "          g_opt.zero_grad()\n",
        "          noise = get_noise(data.shape[0], z_dim, device=device)\n",
        "          fake_data = gan.generator(noise)\n",
        "          loss_generator = -torch.mean(gan.discriminator(fake_data))\n",
        "          loss_generator.backward()\n",
        "          wandb.log({\"loss_generator\": loss_generator})\n",
        "          g_opt.step()\n",
        "\n",
        "        total_loss_discriminator += loss_discriminator.item()\n",
        "        total_loss_generator += loss_generator.item()\n",
        "\n",
        "    num_iterations = len(GEX_dataloader_train)\n",
        "    mean_loss_discriminator = total_loss_discriminator / num_iterations\n",
        "    mean_loss_generator = total_loss_generator / num_iterations\n",
        "\n",
        "    return mean_loss_discriminator, mean_loss_generator\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9jUQGm-sUK3_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_gp(discriminator, real_data, fake_data, batch_size, device):\n",
        "        eps = torch.rand(batch_size,1).to(device)\n",
        "        # print(f\"Batch size {batch_size} real data size {real_data.shape}, eps size {eps.shape}\")\n",
        "        eps = eps.expand_as(real_data).to(device)\n",
        "        # Interpolation between real data and fake data.\n",
        "        interpolation = eps * real_data + (1 - eps) * fake_data\n",
        "        \n",
        "        # get logits for interpolated images\n",
        "        interp_logits = discriminator(interpolation)\n",
        "        grad_outputs = torch.ones_like(interp_logits)\n",
        "        \n",
        "        # Compute Gradients\n",
        "        gradients = torch.autograd.grad(\n",
        "            outputs=interp_logits,\n",
        "            inputs=interpolation,\n",
        "            grad_outputs=grad_outputs,\n",
        "            create_graph=True,\n",
        "            retain_graph=True,\n",
        "        )[0]\n",
        "        \n",
        "        # Compute and return Gradient Norm\n",
        "        gradients = gradients.view(batch_size, -1)\n",
        "        grad_norm = ((torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12) - 1) ** 2).mean() # Add manual calcualtion with epsilon term for stability (Derivatives of the gradient close to 0 can cause problems)\n",
        "        # grad_norm = gradients.norm(2, 1)\n",
        "        return torch.mean((grad_norm - 1) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "l0PHT5MGVJwK"
      },
      "outputs": [],
      "source": [
        "def train_gan_one_epoch_wasserstein_GP(epoch, GEX_dataloader_train, gan, g_opt, d_opt, loss_fn, z_dim, n_disc, w_gp, batch_size, device):\n",
        "    gan.generator.to(device)\n",
        "    gan.discriminator.to(device)\n",
        "    gan.train()\n",
        "    total_loss_discriminator = 0\n",
        "    total_loss_generator = 0\n",
        "    for iteration, (data, _) in enumerate(GEX_dataloader_train):    \n",
        "        real_data = data.to(device)\n",
        "        # Generate fake data\n",
        "        noise = get_noise(batch_size, z_dim, device=device)\n",
        "        fake_data = gan.generator(noise)\n",
        "\n",
        "\n",
        "        fake_data = fake_data.to(device)\n",
        "\n",
        "\n",
        "        # Train the discriminator\n",
        "        d_opt.zero_grad()\n",
        "        pred_real = gan.discriminator(real_data)\n",
        "\n",
        "        gradient_penalty = w_gp * compute_gp(discriminator = gan.discriminator, real_data=real_data, fake_data=fake_data, batch_size=batch_size, device=device)\n",
        "\n",
        "        loss_discriminator = torch.mean(gan.discriminator(fake_data)) - torch.mean(gan.discriminator(real_data))\n",
        "        loss_discriminator = loss_discriminator + gradient_penalty\n",
        "\n",
        "        loss_discriminator.backward(retain_graph=True)\n",
        "\n",
        "        wandb.log({\"loss_discriminator\": loss_discriminator})\n",
        "        wandb.log({\"Gradient penaty\": gradient_penalty})\n",
        "        d_opt.step()\n",
        "\n",
        "        # Train the generator\n",
        "        if iteration % n_disc == 0:\n",
        "          g_opt.zero_grad()\n",
        "          noise = get_noise(batch_size, z_dim, device=device)\n",
        "          fake_data = gan.generator(noise)\n",
        "          loss_generator = -torch.mean(gan.discriminator(fake_data))\n",
        "          loss_generator.backward()\n",
        "          wandb.log({\"loss_generator\": loss_generator})\n",
        "          g_opt.step()\n",
        "\n",
        "        total_loss_discriminator += loss_discriminator.item()\n",
        "        total_loss_generator += loss_generator.item()\n",
        "\n",
        "    num_iterations = len(GEX_dataloader_train)\n",
        "    mean_loss_discriminator = total_loss_discriminator / num_iterations\n",
        "    mean_loss_generator = total_loss_generator / num_iterations\n",
        "\n",
        "    return mean_loss_discriminator, mean_loss_generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "YzQ5OvBJGCWj"
      },
      "outputs": [],
      "source": [
        "config = wandb.config = {\n",
        "    \"lr\": 0.001,\n",
        "    \"batch_size\": 1024,\n",
        "    \"epochs\": 2000,\n",
        "    \"random_seed\": 42,\n",
        "    \"dropout_G\": 0,\n",
        "    \"dropout_D\": 0.2,\n",
        "    \"split_criteria\": \"cell_type\",\n",
        "    \"eval_size_percentage\": 0.2,\n",
        "    \"hidden_sizes_G\": [128, 256, 512],\n",
        "    \"hidden_sizes_D\": [128,64, 32],\n",
        "    \"use_batch_norm_G\": True,\n",
        "    \"use_batch_norm_D\": True,\n",
        "    \"z_dim\": 256,\n",
        "    \"clip_value\" : 0.01,\n",
        "    \"n_disc\" : 4,\n",
        "    \"w_gp\" : 10,\n",
        "    \"device\" : device \n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTQJL6rf-GRS",
        "outputId": "42b155c3-5103-4886-abfa-e5ac00b222c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['B1 B', 'CD4+ T activated', 'CD4+ T naive', 'CD8+ T', 'CD8+ T naive', 'CD14+ Mono', 'CD16+ Mono', 'Erythroblast', 'G/M prog', 'HSC', 'ID2-hi myeloid prog', 'ILC', 'Lymph prog', 'MK/E prog', 'NK', 'Naive CD20+ B', 'Normoblast', 'Plasma cell', 'Proerythroblast', 'Transitional B', 'cDC2', 'pDC']\n",
            "['s1d1', 's1d2', 's1d3', 's2d1', 's2d4', 's2d5', 's3d3', 's3d6', 's3d7', 's3d10', 's4d1', 's4d8', 's4d9']\n"
          ]
        }
      ],
      "source": [
        "print(scdata.obs[\"cell_type\"].cat.categories.tolist())\n",
        "print(scdata.obs[\"batch\"].cat.categories.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AhfFLgLZz0iY"
      },
      "outputs": [],
      "source": [
        "def train_func(config):\n",
        "    run = wandb.init(project=\"single cell integration\", entity=\"eamomc\", config=config)\n",
        "\n",
        "    config = wandb.config\n",
        "    train, test = stratified_split(scdata, config.eval_size_percentage, config.random_seed, split_criteria=config.split_criteria)\n",
        "    train_data = scdata[train]\n",
        "    test_data = scdata[test]\n",
        "    test_data_ = test_data.copy()\n",
        "    test_data_ann = test_data\n",
        "    wandb.log({\"train_size\": len(train_data), \"test_size\": len(test_data)})\n",
        "    \n",
        "\n",
        "\n",
        "    GEX_Dataset_train = GEX_Dataset(train_data, scaler=\"Standard\", cat_var=\"batch\", label_encoder=\"one_hot\")\n",
        "    GEX_dataloader_train = torch.utils.data.DataLoader(GEX_Dataset_train, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    GEX_Dataset_test = GEX_Dataset(test_data, scaler=\"Standard\", cat_var=\"batch\", label_encoder=\"one_hot\")\n",
        "\n",
        "    generator = Generator(output_size=GEX_Dataset_train.n_features, input_size=config.z_dim, hidden_sizes=config.hidden_sizes_G, dropout=config.dropout_G, use_batch_norm=config.use_batch_norm_G).to(config.device)\n",
        "    discriminator = Discriminator(input_size=GEX_Dataset_train.n_features, output_size=1, hidden_sizes=config.hidden_sizes_D, use_batch_norm=config.use_batch_norm_D, dropout=config.dropout_D).to(config.device)\n",
        "\n",
        "    gan = GAN(generator, discriminator).to(config.device)\n",
        "    print(gan)\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    d_opt = torch.optim.Adam(gan.discriminator.parameters(), lr=config.lr)\n",
        "    g_opt = torch.optim.Adam(gan.generator.parameters(), lr=config.lr)\n",
        "\n",
        " \n",
        "\n",
        "    data = np.array(GEX_Dataset_test[:])[0]\n",
        "    test_data_ann.X = np.array(data)\n",
        "    \n",
        "    \n",
        "    hyg_train = get_hvg_set(train_data, n_top_genes=2000)\n",
        "    hvg_test =  get_hvg_set(test_data, n_top_genes=2000)\n",
        "\n",
        "    # test_data_ = test_data\n",
        "    # sc.pp.neighbors(test_data_ann, n_neighbors=10)\n",
        "    # sc.tl.umap(test_data_ann)\n",
        "    # test_UMAP_batch = sc.pl.umap(test_data_ann, show=False, color=\"batch\")\n",
        "    # test_UMAP_cell_type = sc.pl.umap(test_data_ann, show=False, color=\"cell_type\")\n",
        "\n",
        "    # wandb.log({\"Test UMAP baseline batch\": wandb.Image(test_UMAP_batch)})\n",
        "    # wandb.log({\"Test UMAP baseline cell type\": wandb.Image(test_UMAP_cell_type)})\n",
        "\n",
        "    unique_cell_types = umap_data.obs[\"cell_type\"].unique().tolist()\n",
        "    unique_batches = umap_data.obs[\"batch\"].unique().tolist()\n",
        "    # Define custom color dictionaries for cell types and batches\n",
        "    custom_palette_cell_type = {cell_type: ('#00FF00' if cell_type == \"fake\" else None) for cell_type in unique_cell_types}\n",
        "    custom_palette_batch = {batch: ('#00FF00' if batch == \"fake\" else None) for batch in unique_batches}\n",
        "\n",
        "\n",
        "    gan.train()\n",
        "    for epoch in tqdm(range(1, config.epochs + 1)):\n",
        "        wandb.log({\"epoch\": epoch})\n",
        "        D_loss_train, G_loss_train = train_gan_one_epoch_wasserstein_GP(epoch=epoch, gan=gan, GEX_dataloader_train=GEX_dataloader_train, d_opt=d_opt, g_opt=g_opt, loss_fn=loss_fn, z_dim=config.z_dim, batch_size=config.batch_size, device=config.device, n_disc=config.n_disc, w_gp=config.w_gp)\n",
        "        wandb.log({\"D_loss_train\": D_loss_train, \"G_loss_train\": G_loss_train})\n",
        "\n",
        "    eval_condition = (\n",
        "          (epoch <= 10 and epoch % 2 == 0) or\n",
        "          (10 < epoch <= 100 and epoch % 25 == 0) or\n",
        "          (100 < epoch <= 1000 and epoch % 100 == 0) or\n",
        "          (1000 < epoch <= 2000 and epoch % 200 == 0)\n",
        "      )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if eval_condition:\n",
        "      umap_data = generate_fake_data_and_combine(test_data_ann, generator, config.z_dim, config.batch_size)\n",
        "      umap_data_fake = umap_data[umap_data.obs[\"group\"] == \"fake\"].copy()\n",
        "      jaccard_train, jaccard_test = hvg_overlap_analysis(hyg_train, hvg_test, umap_data_fake, GEX_Dataset_train, n_top_genes=2000)\n",
        "      wandb.log({\"Jaccard Train\": jaccard_train, \"Jaccard Test\": jaccard_test})\n",
        "\n",
        "      umap_real_and_fake = sc.pl.umap(umap_data, color=\"group\", show=False, title=f\"UMAP Real and Fake (Epoch {epoch})\", palette=custom_palette_cell_type)\n",
        "      umap_batch = sc.pl.umap(umap_data, color=\"batch\", show=False, title=f\"UMAP Batch (Epoch {epoch})\", palette=custom_palette_batch)\n",
        "      umap_cell_type = sc.pl.umap(umap_data, color=\"cell_type\", show=False, title=f\"UMAP Cell Type (Epoch {epoch})\", palette=custom_palette_cell_type)\n",
        "\n",
        "\n",
        "    \n",
        "      # Save UMAP images to temporary files\n",
        "      with tempfile.NamedTemporaryFile(suffix=\".png\") as tmp_umap_real_and_fake:\n",
        "          umap_real_and_fake.figure.savefig(tmp_umap_real_and_fake.name, dpi=300, bbox_inches='tight')\n",
        "          wandb.log({\"UMAP Real and Fake\": wandb.Image(tmp_umap_real_and_fake.name)})\n",
        "\n",
        "      with tempfile.NamedTemporaryFile(suffix=\".png\") as tmp_umap_batch:\n",
        "          umap_batch.figure.savefig(tmp_umap_batch.name, dpi=300, bbox_inches='tight')\n",
        "          wandb.log({\"UMAP Batch\": wandb.Image(tmp_umap_batch.name)})\n",
        "\n",
        "      with tempfile.NamedTemporaryFile(suffix=\".png\") as tmp_umap_cell_type:\n",
        "          umap_cell_type.figure.savefig(tmp_umap_cell_type.name, dpi=300, bbox_inches='tight')\n",
        "          wandb.log({\"UMAP Cell Type\": wandb.Image(tmp_umap_cell_type.name)})\n",
        "\n",
        "      \n",
        "      if epoch > 100:\n",
        "        generator_weights_path = os.path.join(wandb.run.dir, f\"generator_epoch_{epoch}.pt\")\n",
        "        discriminator_weights_path = os.path.join(wandb.run.dir, f\"discriminator_epoch_{epoch}.pt\")\n",
        "        torch.save(gan.generator.state_dict(), generator_weights_path)\n",
        "        torch.save(gan.discriminator.state_dict(), discriminator_weights_path)\n",
        "\n",
        "        # Log the model weights to wandb\n",
        "        wandb.save(generator_weights_path)\n",
        "        wandb.save(discriminator_weights_path)\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "\n",
        "       \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "id": "SCnHz64fuJkZ",
        "outputId": "3cdc2070-c44c-4016-9078-dfbff7dd544c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meamomc\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230416_132144-wd2zf7ol</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/eamomc/single%20cell%20integration/runs/wd2zf7ol' target=\"_blank\">treasured-cosmos-36</a></strong> to <a href='https://wandb.ai/eamomc/single%20cell%20integration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/eamomc/single%20cell%20integration' target=\"_blank\">https://wandb.ai/eamomc/single%20cell%20integration</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/eamomc/single%20cell%20integration/runs/wd2zf7ol' target=\"_blank\">https://wandb.ai/eamomc/single%20cell%20integration/runs/wd2zf7ol</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAN(\n",
            "  (generator): Generator(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
            "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "      (3): Linear(in_features=128, out_features=256, bias=True)\n",
            "      (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU()\n",
            "      (6): Linear(in_features=256, out_features=512, bias=True)\n",
            "      (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): ReLU()\n",
            "      (9): Linear(in_features=512, out_features=13431, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (discriminator): Discriminator(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=13431, out_features=128, bias=True)\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "      (2): Dropout(p=0.2, inplace=False)\n",
            "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "      (4): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      (5): LeakyReLU(negative_slope=0.2)\n",
            "      (6): Dropout(p=0.2, inplace=False)\n",
            "      (7): Linear(in_features=64, out_features=32, bias=True)\n",
            "      (8): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      (9): LeakyReLU(negative_slope=0.2)\n",
            "      (10): Dropout(p=0.2, inplace=False)\n",
            "      (11): Linear(in_features=32, out_features=1, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-e03457f0f497>:31: FutureWarning:\n",
            "\n",
            "The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
            "\n",
            "<ipython-input-29-e03457f0f497>:31: VisibleDeprecationWarning:\n",
            "\n",
            "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "\n",
            "/usr/local/lib/python3.9/dist-packages/scipy/sparse/_index.py:137: SparseEfficiencyWarning:\n",
            "\n",
            "Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_func(config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UECkjDwVIo3o"
      },
      "outputs": [],
      "source": [
        "config = wandb.config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHN4J-I1k1fL"
      },
      "outputs": [],
      "source": [
        "config = wandb.config\n",
        "train, test = stratified_split(scdata, config.eval_size_percentage, config.random_seed, split_criteria=config.split_criteria)\n",
        "train_data = scdata[train]\n",
        "test_data = scdata[test]\n",
        "test_data_ = test_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.X.toarray()"
      ],
      "metadata": {
        "id": "Jb6MqgelirDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.X.toarray()"
      ],
      "metadata": {
        "id": "Ni4wr_KOitIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bo_ilAIbjmXk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('scINTEGRATION')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "1d226e3599a48bcd2e3e064e4b49e64b5c23bb1e3c85e4572c7816e0051bede7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}